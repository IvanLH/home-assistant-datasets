---
# This is the model configuration file used by the `home-assistant-datasets`
# tool. Each entry is a ConfigEntry that configures a specific model type
# with whatever integration is needed (either core integration or custom component).
#
# Some models require secrets such as API keys or urls that are specific to you
# and not checked into the repository and referenced in this file like `!secret openai_api_key`.
# Create a file called `secrets.yaml` and include the necessary keys.
models:
  - model_id: assistant
    domain: homeassistant

  - model_id: gpt-3.5
    domain: openai_conversation
    config_entry_data:
      api_key: !secret openai_api_key
    config_entry_options:
      chat_model: gpt-3.5-turbo-1106
      llm_hass_api: assist

  - model_id: gpt-4o
    domain: openai_conversation
    config_entry_data:
      api_key: !secret openai_api_key
    config_entry_options:
      chat_model: gpt-4o
      llm_hass_api: assist

  - model_id: gpt-4o-mini
    domain: openai_conversation
    config_entry_data:
      api_key: !secret openai_api_key
    config_entry_options:
      chat_model: gpt-4o-mini
      llm_hass_api: assist

  - model_id: gemini-pro
    domain: google_generative_ai_conversation
    config_entry_data:
      api_key: !secret google_api_key
    config_entry_options:
      chat_model: models/gemini-pro
      llm_hass_api: assist

  - model_id: gemini-1.5-flash
    domain: google_generative_ai_conversation
    config_entry_data:
      api_key: !secret google_api_key
    config_entry_options:
      chat_model: models/gemini-1.5-flash-latest
      llm_hass_api: assist

  # Using an openai compatible custom component
  # https://github.com/allenporter/hass-openai-custom-conversation
  - model_id: functionary-small-v2.5
    domain: vicuna_conversation
    config_entry_data:
      api_key: sk-0000000000000000000
      # Update with your openai compatible functionary url
      base_url: http://functionary.functionary:8000/v1
    config_entry_options:
      chat_model: functionary-small-v2.5
      llm_hass_api: assist

  - model_id: mistral-v3
    domain: ollama
    config_entry_data:
      url: !secret ollama_url
      model: mistral:latest
    config_entry_options:
      llm_hass_api: assist

  - model_id: llama3
    domain: ollama
    config_entry_data:
      url: !secret ollama_url
      model: llama3:latest

  - model_id: llama3.1
    domain: ollama
    config_entry_data:
      url: !secret ollama_url
      model: llama3.1:latest
    config_entry_options:
      llm_hass_api: assist

  - model_id: xlam-1b
    domain: ollama
    config_entry_data:
      url: !secret ollama_url
      model: allenporter/xlam:1b
    config_entry_options:
      llm_hass_api: assist

  - model_id: xlam-7b
    domain: ollama
    config_entry_data:
      url: !secret ollama_url
      model: allenporter/xlam:7b
    config_entry_options:
      llm_hass_api: assist

  - model_id: gemma
    domain: ollama
    config_entry_data:
      url: !secret ollama_url
      model: gemma:7b

  - model_id: llama3-groq-tool-use
    domain: ollama
    config_entry_data:
      url: !secret ollama_url
      model: llama3-groq-tool-use:latest
    config_entry_options:
      llm_hass_api: assist

  # Configuration for home-llm custom component https://github.com/acon96/home-llm
  - model_id: home-llm
    domain: llama_conversation
    version: 2
    # Home LLM has a very complex configuration that depends on the specific backend
    # and model used. You're on your own to figure out the valid values to enter here.
    config_entry_data:
      model_backend: ollama
      host: ollama.ollama
      port: "11434"
      ssl: false
      huggingface_model: fixt/home-3b-v3:latest
    config_entry_options:
      llm_hass_api: home-llm-service-api
      prompt: "You are 'Al', a helpful AI Assistant that controls the devices in a house. Complete the following task as instructed with the information provided only.\nThe current time and date is {{ (as_timestamp(now()) | timestamp_custom(\"%I:%M %p on %A %B %d, %Y\", \"\")) }}\nServices: {{ formatted_tools }}\nDevices:\n{{ formatted_devices }}"
      prompt_template: "zephyr"
      tool_format: "min_tool_format"
      tool_multi_turn_chat: false
      in_context_examples: false
      in_context_examples_file: "in_context_examples.csv"
      num_in_context_examples: 4.0
      max_new_tokens: 128
      context_length: 2048.0
      top_k: 40.0
      temperature: 0.1
      top_p: 1.0
      typical_p: 1.0
      ollama_json_mode: false
      request_timeout: 90.0
      ollama_keep_alive: 30.0
      remote_use_chat_endpoint: false
      extra_attributes_to_expose:
        - rgb_color
        - brightness
        - temperature
        - humidity
        - fan_mode
        - media_title
        - volume_level
        - item
        - wind_speed
      service_call_regex: "```homeassistant\\n([\\S \\t\\n]*?)```"
      refresh_prompt_per_turn: true
      remember_conversation: true
      remember_num_interactions: 5
