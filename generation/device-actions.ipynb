{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Actions\n",
    "\n",
    "Generates datasets for performing actions on devices in a synthetic home. This will generate a list\n",
    "of text / voice commands that you can perform in a home. These are not labeled with the outcome\n",
    "which is generated in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import google.generativeai as genai\n",
    "\n",
    "from home_assistant_datasets import secrets\n",
    "from home_assistant_datasets.secrets import get_secret\n",
    "from home_assistant_datasets import model_client\n",
    "\n",
    "secrets.DEFAULT_SECRETS_FILE = \"../secrets.yaml\"\n",
    "\n",
    "# MODEL_ID = \"gpt-3.5-turbo-0125\"\n",
    "# openai = openai.OpenAI(api_key=secrets.get_secret(\"openai_api_key\"))\n",
    "# model = model_client.ModelClient(openai, MODEL_ID)\n",
    "\n",
    "# Gemini flash is higher quality and cheaper model than the GPT alternatives.\n",
    "MODEL_ID = \"gemini-1.5-flash\"\n",
    "genai.configure(api_key=secrets.get_secret(\"google_api_key\"))\n",
    "model = model_client.GoogleClient(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate few-shot examples\n",
    "\n",
    "Read the seed data used as a few-shot example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "home: mountain-cabin-us\n",
      "device:\n",
      "  name: Kitchen Overhead Light\n",
      "  area: Kitchen\n",
      "  device_type: light\n",
      "  device_info:\n",
      "    model: Smart LED Bulb\n",
      "    manufacturer: Philips\n",
      "    sw_version: 1.2.3\n",
      "capabilities:\n",
      "- Turn on\n",
      "- Turn off\n",
      "---\n",
      "actions:\n",
      "- action: Turn on\n",
      "  sentences:\n",
      "  - Please turn on the kitchen overhead light\n",
      "  - Turn on the kitchen light\n",
      "  - Kitchen light on\n",
      "- action: Turn off\n",
      "  sentences:\n",
      "  - Please turn off the kitchen overhead light\n",
      "  - Turn off the kitchen light\n",
      "  - Kitchen light off\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "from synthetic_home import device_types\n",
    "\n",
    "\n",
    "DATASET_DIR = pathlib.Path(\"../datasets/\")\n",
    "DEVICES_DIR = DATASET_DIR / \"devices-v3\"\n",
    "SEEDS_DIR = pathlib.Path(\"./seeds\")\n",
    "SEED_DEVICE_ACTIONS_FILE = SEEDS_DIR / \"device-actions.yaml\"\n",
    "SEED_DEVICE_ACTIONS_CAPABILITIES_FILE = SEEDS_DIR / \"device-actions-capabilities.yaml\"\n",
    "\n",
    "with open(SEED_DEVICE_ACTIONS_FILE) as f:\n",
    "    seed_device_actions = list(yaml.load_all(f.read(), Loader=yaml.Loader))\n",
    "\n",
    "# This is a fixed list of capabilities that any particular synthetic home device type support\n",
    "with open(SEED_DEVICE_ACTIONS_CAPABILITIES_FILE) as f:\n",
    "    capabilities = {\n",
    "        cap[\"device_type\"]: cap[\"actions\"]\n",
    "        for cap in yaml.load(f.read(), Loader=yaml.Loader)\n",
    "    }\n",
    "\n",
    "seed_devices_prompt = \"\".join(yaml.dump(content, sort_keys=False, explicit_start=True) for content in seed_device_actions)\n",
    "print(seed_devices_prompt)\n",
    "\n",
    "registry = device_types.load_device_type_registry()\n",
    "# Find any devices missing explicit action capabilities definitions\n",
    "missing_devices = [\n",
    "    {\"device_type\": dt, \"actions\": []}\n",
    "    for dt in registry.device_types\n",
    "    if dt not in capabilities\n",
    "]\n",
    "if missing_devices:\n",
    "    print(yaml.dump(missing_devices, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_PROMPT = f\"\"\"\n",
    "You are an expert Smart Home agent who can evaluate the performance of a smart\n",
    "home, and perform useful actions on behalf of a user.\n",
    "\n",
    "A device in Home Assistant represents a physical or virtual object, represented\n",
    "by different entities. A device has attributes for its configuration and state,\n",
    "for example a thermostat may have a mode attribute, or target or current temperature\n",
    "attributes.\n",
    "\n",
    "You generate a simple evaluation dataset for home data. The input dataset\n",
    "contains the home, description information like location, areas, and devices.\n",
    "The output data are actions a user may ask to take on a devie.\n",
    "\n",
    "This is the input yaml document and the output actions yaml document:\n",
    "\n",
    "{seed_devices_prompt}\n",
    "\n",
    "Generate a few sentences to control the device. Answer in yaml plain text and do not answer with markdown.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 480, 91)\n",
      "exhaust-fan: 11\n",
      "fan-oscilating: 2\n",
      "garage-door: 6\n",
      "heat-pump: 3\n",
      "hvac: 30\n",
      "light: 203\n",
      "light-dimmable: 85\n",
      "smart-blinds: 1\n",
      "smart-lock: 5\n",
      "smart-plug: 34\n",
      "smart-speaker: 49\n",
      "smart-sprinkler: 17\n",
      "smart-tv: 17\n",
      "switch: 8\n",
      "vacuum: 3\n",
      "water-valve: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "import slugify\n",
    "\n",
    "homes = []\n",
    "for path in DEVICES_DIR.glob(\"*.yaml\"):\n",
    "    with path.open(\"r\") as f:\n",
    "        content = f.read()\n",
    "    home_id = path.name.split(\".\")[0]  # Strip the .yaml extension\n",
    "    home_data = yaml.load(content, Loader=yaml.Loader)\n",
    "    homes.append((home_id, home_data))\n",
    "\n",
    "tasks = []\n",
    "no_actions = 0\n",
    "task_types = {}\n",
    "for home_id, home in homes:\n",
    "    home_template = {\n",
    "            \"home\": home_id,\n",
    "            \"location\": home[\"location\"],\n",
    "            \"type\": home[\"type\"],\n",
    "    }\n",
    "    for area, devices in home[\"devices\"].items():\n",
    "        for device in devices or []:\n",
    "            device_type = device[\"device_type\"]\n",
    "            if not (device_caps := capabilities.get(device_type)):\n",
    "                # No supported actions\n",
    "                no_actions += 1\n",
    "                continue\n",
    "            task_types[device_type] = task_types.get(device_type, 0) + 1\n",
    "            device_info = {\n",
    "                    **home_template,\n",
    "                    \"device\": {\n",
    "                        **device,\n",
    "                        \"area\": area,\n",
    "                    },\n",
    "                    \"capabilities\": device_caps,\n",
    "            }\n",
    "            tasks.append(device_info)\n",
    "print((len(homes), len(tasks), no_actions))\n",
    "print(yaml.dump(task_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "home: home4-us\n",
      "location: Coastal town in Florida\n",
      "type: Beach house\n",
      "device:\n",
      "  name: Kids Bathroom Light\n",
      "  device_type: light\n",
      "  device_info:\n",
      "    model: Smart LED Bulb\n",
      "    manufacturer: Philips\n",
      "    sw_version: 1.2.3\n",
      "  area: Kids Bathroom\n",
      "capabilities:\n",
      "- Turn on\n",
      "- Turn off\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(tasks)\n",
    "print(yaml.dump(tasks[0], sort_keys=False, explicit_start=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 0:  10%|█         | 48/480 [02:13<15:24,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 20, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 1:  14%|█▍        | 69/480 [03:14<19:18,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 13, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 2:  21%|██        | 99/480 [04:35<16:23,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 13, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 4:  33%|███▎      | 158/480 [07:12<13:14,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 13, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 4:  39%|███▉      | 186/480 [08:47<12:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 19, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 6:  45%|████▌     | 217/480 [10:06<11:23,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 22, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 6:  57%|█████▋    | 272/480 [12:30<07:35,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while parsing a block mapping\n",
      "  in \"<unicode string>\", line 13, column 3:\n",
      "    - action: Set position\n",
      "      ^\n",
      "expected <block end>, but found '<scalar>'\n",
      "  in \"<unicode string>\", line 17, column 16:\n",
      "      - [position] the shower\n",
      "                   ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 8:  66%|██████▋   | 318/480 [14:27<07:41,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 15, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 9:  80%|████████  | 386/480 [17:46<04:14,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected a single document in the stream\n",
      "  in \"<unicode string>\", line 2, column 1:\n",
      "    actions:\n",
      "    ^\n",
      "but found another document\n",
      "  in \"<unicode string>\", line 21, column 1:\n",
      "    ---\n",
      "    ^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped 9: 100%|██████████| 480/480 [22:00<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "import slugify\n",
    "\n",
    "# Total number of records to generate\n",
    "N_DATAPOINTS = -1\n",
    "\n",
    "DEVICE_ACTIONS_OUTPUT_DIR = DATASET_DIR / \"device-actions-v2\"\n",
    "\n",
    "# Wipe existing summaries\n",
    "shutil.rmtree(DEVICE_ACTIONS_OUTPUT_DIR, ignore_errors=True)\n",
    "DEVICE_ACTIONS_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "random.shuffle(tasks)\n",
    "if N_DATAPOINTS > 0 and len(tasks) > N_DATAPOINTS:\n",
    "    tasks = tasks[:N_DATAPOINTS]\n",
    "\n",
    "skipped = 0\n",
    "with tqdm(total=len(tasks)) as pbar:\n",
    "    for task in tasks:\n",
    "        home_id = slugify.slugify(task[\"home\"], separator=\"-\")\n",
    "        task_id = \"_\".join([\n",
    "              slugify.slugify(task[\"device\"][\"area\"], separator=\"-\"),\n",
    "              slugify.slugify(task[\"device\"][\"name\"], separator=\"-\"),\n",
    "        ])\n",
    "        home_dir = DEVICE_ACTIONS_OUTPUT_DIR / home_id\n",
    "        if not home_dir.exists():\n",
    "            home_dir.mkdir()\n",
    "        with open(DEVICE_ACTIONS_OUTPUT_DIR / home_id / f\"{task_id}.yaml\", \"w\") as action_output:\n",
    "            task_yaml = yaml.dump(task, sort_keys=False, explicit_start=True)\n",
    "            response_obj = None\n",
    "            for i in range(3):\n",
    "                response = model.complete(SUMMARY_PROMPT, task_yaml)\n",
    "                try:\n",
    "                    response_obj = yaml.safe_load(response)\n",
    "                except yaml.YAMLError as err:\n",
    "                    print(err)\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "            if response_obj is not None:\n",
    "                updated_task = task.copy()\n",
    "                updated_task.update({\"actions\": response_obj})\n",
    "                action_output.write(yaml.dump(updated_task, explicit_start=True, sort_keys=False))\n",
    "            pbar.set_description(f\"Skipped {skipped}\")\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Actions Fixtures\n",
    "\n",
    "Generate test fixtures from the device actions datasets. This will create the\n",
    "home inventory to power the device actions data collections steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 479 1908\n",
      "---\n",
      "light: 619\n",
      "smart-plug: 109\n",
      "light-dimmable: 636\n",
      "hvac: 103\n",
      "exhaust-fan: 33\n",
      "smart-speaker: 141\n",
      "switch: 25\n",
      "smart-sprinkler: 56\n",
      "water-valve: 28\n",
      "garage-door: 18\n",
      "smart-tv: 95\n",
      "heat-pump: 10\n",
      "smart-lock: 15\n",
      "smart-blinds: 3\n",
      "vacuum: 11\n",
      "fan-oscilating: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "import pathlib\n",
    "from synthetic_home import synthetic_home\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "DATASET_DIR = pathlib.Path(\"../datasets/\")\n",
    "DEVICES_DIR = DATASET_DIR / \"devices-v3\"\n",
    "DEVICE_ACTIONS_DIR = DATASET_DIR / \"device-actions-v2\"\n",
    "DEVICE_ACTIONS_FIXTURES_DIR = DATASET_DIR / \"device-actions-v2-fixtures\"\n",
    "\n",
    "shutil.rmtree(DEVICE_ACTIONS_FIXTURES_DIR, ignore_errors=True)\n",
    "DEVICE_ACTIONS_FIXTURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "homes_count = 0\n",
    "devices_count = 0\n",
    "sentences_count = 0\n",
    "device_type_sentences = {}\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DeviceTasks:\n",
    "   device: str\n",
    "   area: str | None\n",
    "   device_id: str | None\n",
    "   entity_id: str | None\n",
    "   sentences: list[str]\n",
    "\n",
    "\n",
    "for devices_file in DEVICES_DIR.glob(\"*.yaml\"):\n",
    "   home_id = devices_file.name.split(\".\")[0]\n",
    "   home = synthetic_home.load_synthetic_home(devices_file)\n",
    "\n",
    "   home_dir = DEVICE_ACTIONS_FIXTURES_DIR / home_id\n",
    "   home_dir.mkdir(exist_ok=True)\n",
    "\n",
    "   inventory = synthetic_home.build_inventory(home)\n",
    "\n",
    "   fixtures = home_dir / \"_fixtures.yaml\"\n",
    "   fixtures.write_text(inventory.to_yaml())\n",
    "\n",
    "   homes_count += 1\n",
    "   category_tasks = {}\n",
    "   for actions_file in (DEVICE_ACTIONS_DIR / home_id).glob(\"*.yaml\"):\n",
    "      devices_count += 1\n",
    "      device_actions = yaml.load(actions_file.read_text(), Loader=yaml.CSafeLoader)\n",
    "      device = device_actions[\"device\"]\n",
    "      category = device[\"device_type\"]\n",
    "      if category not in category_tasks:\n",
    "         category_tasks[category] = []\n",
    "\n",
    "      device_id: str | None = None\n",
    "      for inv_device in inventory.devices:\n",
    "         if inv_device.name.lower() == device[\"name\"].lower():\n",
    "            device_id = inv_device.id\n",
    "            break\n",
    "      assert device_id\n",
    "      entity_id: str | None = None\n",
    "      for inv_entity in inventory.entities:\n",
    "         if inv_entity.name.lower() == device[\"name\"].lower():\n",
    "            if inv_entity.device != device_id:\n",
    "               raise ValueError(f\"Wrong device: {device}\")\n",
    "            entity_id = inv_entity.id\n",
    "            break\n",
    "      assert entity_id\n",
    "      if entity_id.startswith(\"sensor\") or entity_id.startswith(\"binary_sensor\"):\n",
    "         raise ValueError(f\"Matched entity that does not support control {device}\")\n",
    "\n",
    "      actions = device_actions[\"actions\"]\n",
    "      if \"actions\" in actions:\n",
    "         actions = actions[\"actions\"]\n",
    "      for action_data in actions:\n",
    "         sentences = action_data[\"sentences\"]\n",
    "         category_tasks[category].append(\n",
    "            DeviceTasks(device=device[\"name\"], area=device[\"area\"], device_id=device_id, entity_id=entity_id, sentences=sentences)\n",
    "         )\n",
    "      sentences_count += len(sentences)\n",
    "      device_type_sentences[category] = device_type_sentences.get(category, 0) + len(sentences)\n",
    "\n",
    "   for category, tasks in category_tasks.items():\n",
    "      data = {\n",
    "         \"category\": category,\n",
    "         \"tests\": [\n",
    "            dataclasses.asdict(task)\n",
    "            for task in tasks\n",
    "         ]\n",
    "      }\n",
    "      category_file = home_dir / f\"{category}.yaml\"\n",
    "      category_file.write_text(yaml.dump(data, sort_keys=False, explicit_start=True))\n",
    "\n",
    "print(homes_count, devices_count, sentences_count)\n",
    "print(yaml.dump(device_type_sentences, explicit_start=True, sort_keys=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assist pipeline teacher\n",
    "\n",
    "Run the assist pipeline data collection step to generate tool calls against the fixtures\n",
    "\n",
    "```bash\n",
    "$ source venv/bin/activate\n",
    "(venv) $ home-assistant-datasets assist collect --dataset ./datasets/device-actions-v2-fixtures/ --model_output_dir=./datasets/device-actions-v2-collect/ --models=assistant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save successful Assistant results\n",
    "\n",
    "This saves all the successful results from the assistant pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "import slugify\n",
    "import shutil\n",
    "\n",
    "DATASET_DIR = pathlib.Path(\"../datasets/\")\n",
    "FIXTTURES_DIR = DATASET_DIR / \"device-actions-v2-fixtures\"\n",
    "COLLECT_DIR = DATASET_DIR / \"device-actions-v2-collect\"\n",
    "ASSIST_TEACHER_DIR = COLLECT_DIR / \"assistant\"\n",
    "\n",
    "TRAIN_DIR = COLLECT_DIR / \"train\"\n",
    "shutil.rmtree(TRAIN_DIR, ignore_errors=True)\n",
    "TRAIN_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# These can not be used in the training set since they are used for eval\n",
    "EVAL_HOME_IDS = {\n",
    "  \"dom1-pl\",\n",
    "  \"home1-us\",\n",
    "  \"home2-ru\",\n",
    "  \"home5-cn\",\n",
    "  \"home7-dk\",\n",
    "}\n",
    "\n",
    "\n",
    "success = {}\n",
    "total = {}\n",
    "total_sentences = 0\n",
    "\n",
    "for path in FIXTTURES_DIR.glob(\"**/*.yaml\"):\n",
    "    if path.name == \"_fixtures.yaml\":\n",
    "        continue\n",
    "    home_id = path.parent.name\n",
    "    category = path.name.split(\".\")[0]\n",
    "\n",
    "    if home_id in EVAL_HOME_IDS:\n",
    "        continue\n",
    "\n",
    "    fixture_record = yaml.load(path.read_text(), Loader=yaml.CSafeLoader)\n",
    "    matched_tests = []\n",
    "\n",
    "    file_prefix = \"_\".join([\n",
    "        slugify.slugify(home_id, separator=\"_\"),\n",
    "        slugify.slugify(category, separator=\"_\"),\n",
    "    ])\n",
    "    assist_outputs = ASSIST_TEACHER_DIR.glob(f\"{file_prefix}*.yaml\")\n",
    "    for filename in assist_outputs:\n",
    "        total[category] = total.get(category, 0) + 1\n",
    "        record = yaml.load(filename.read_text(), Loader=yaml.Loader)\n",
    "        input_text = record[\"task\"][\"input_text\"]\n",
    "        if record[\"response\"].startswith(\"Sorry\"):\n",
    "            continue\n",
    "        success[category] = success.get(category, 0) + 1\n",
    "\n",
    "        context = record[\"context\"]\n",
    "        conversation_trace = context[\"conversation_trace\"]\n",
    "        if len(conversation_trace) < 2:\n",
    "            continue\n",
    "        if conversation_trace[1][\"event_type\"] != \"tool_call\":\n",
    "            continue\n",
    "        if not (tool_call := conversation_trace[1].get(\"data\")):\n",
    "            continue\n",
    "\n",
    "        for record in fixture_record[\"tests\"]:\n",
    "            sentences = list(record[\"sentences\"])\n",
    "            for sentence in sentences:\n",
    "                if sentence == input_text:\n",
    "                    total_sentences += 1\n",
    "                    matched_tests.append({\n",
    "                        **(record.copy()),\n",
    "                        \"sentences\": [sentence],\n",
    "                        \"function\": {\n",
    "                            \"name\": tool_call[\"intent_name\"],\n",
    "                            \"arguments\": tool_call[\"slots\"],\n",
    "                        }\n",
    "                    })\n",
    "                    break\n",
    "\n",
    "    if not matched_tests:\n",
    "        continue\n",
    "    output_record = fixture_record.copy()\n",
    "    output_record[\"tests\"] = matched_tests\n",
    "    if output_record[\"tests\"]:\n",
    "        (TRAIN_DIR / home_id).mkdir(exist_ok=True)\n",
    "        out_file = TRAIN_DIR / home_id / f\"{category}.yaml\"\n",
    "        out_file.write_text(yaml.dump(output_record, sort_keys=False, explicit_start=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 2332\n",
      "smart-tv - 26 - 13.54% - 1.11%\n",
      "smart-speaker - 26 - 3.83% - 1.11%\n",
      "light-dimmable - 784 - 67.64% - 33.62%\n",
      "water-valve - 13 - 20.31% - 0.56%\n",
      "smart-plug - 140 - 64.22% - 6.00%\n",
      "light - 1824 - 77.78% - 78.22%\n",
      "exhaust-fan - 66 - 100.00% - 2.83%\n",
      "switch - 28 - 63.64% - 1.20%\n",
      "smart-lock - 16 - 53.33% - 0.69%\n",
      "fan-oscilating - 11 - 91.67% - 0.47%\n",
      "smart-sprinkler - 20 - 17.86% - 0.86%\n",
      "hvac - 100 - 33.33% - 4.29%\n",
      "garage-door - 0 - 0.00% - 0.00%\n",
      "vacuum - 2 - 9.09% - 0.09%\n",
      "heat-pump - 8 - 42.11% - 0.34%\n",
      "smart-blinds - 4 - 66.67% - 0.17%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total sentences: {total_sentences}\")\n",
    "for category in total:\n",
    "    s = success.get(category, 0)\n",
    "    t = total[category]\n",
    "    print(f\"{category} - {s} - {100*(s / t):0.2f}% - {100*(s / total_sentences):0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud LLM Teacher\n",
    "\n",
    "Scrape cloud responses with:\n",
    "\n",
    "```\n",
    "$ home-assistant-datasets assist collect --dataset ./datasets/device-actions-v2-fixtures/ --model_output_dir=./datasets/device-actions-v2-collect/ --models=gemini-1.5-flash\n",
    "```\n",
    "\n",
    "Then convert into lower level system messages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  23%|██▎       | 480/2124 [00:20<01:07, 24.31it/s]"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "import shutil\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import tqdm\n",
    "import random\n",
    "from typing import Any\n",
    "from home_assistant_datasets.tokenizer import conversation, chat_template\n",
    "\n",
    "DATASET_DIR = pathlib.Path(\"../datasets/\")\n",
    "COLLECT_DIR = DATASET_DIR / \"device-actions-v2-collect\"\n",
    "TEACHER_MODEL_DIR = COLLECT_DIR / \"gemini-1.5-flash\"\n",
    "\n",
    "\n",
    "def create_conversation(record: dict[str, Any]) -> conversation.ConversationRecord:\n",
    "    conversation_trace = record[\"context\"][\"conversation_trace\"]\n",
    "\n",
    "    input_detail = next(filter(lambda x: x[\"event_type\"] == \"async_process\", conversation_trace), None)\n",
    "    input_text = input_detail[\"data\"][\"text\"]\n",
    "\n",
    "    agent_detail = next(filter(lambda x: x[\"event_type\"] == \"agent_detail\", conversation_trace), None)\n",
    "    prompt = agent_detail[\"data\"][\"prompt\"]\n",
    "    prompt = \"\\n\".join(prompt.split(\"\\n\")[1:])  # Strip \"Current time is...\"\n",
    "\n",
    "    tool_call_trace = next(filter(lambda x: x[\"event_type\"] == \"tool_call\", conversation_trace), None)\n",
    "    tool_calls: dict[str, str] | None = None\n",
    "    content: str | None = None\n",
    "    if tool_call_trace:\n",
    "        tool_calls = [{\n",
    "            \"name\": tool_call_trace[\"data\"][\"tool_name\"],\n",
    "            \"arguments\": tool_call_trace[\"data\"][\"tool_args\"],\n",
    "        }]\n",
    "    else:\n",
    "        content = record[\"response\"]\n",
    "    message = {\n",
    "        \"instructions\": prompt,\n",
    "        \"tools\": agent_detail[\"data\"][\"tools\"],\n",
    "        \"input\": input_text,\n",
    "        \"output\": content or \"\",\n",
    "        \"tool_calls\": tool_calls or None,\n",
    "    }\n",
    "    return conversation.ConversationRecord.from_dict(message)\n",
    "\n",
    "teacher_files = list(TEACHER_MODEL_DIR.glob(\"*.yaml\"))\n",
    "random.shuffle(teacher_files)\n",
    "\n",
    "def chunk_into_n(lst: list[str], n: int) -> list[list[str]]:\n",
    "  size = math.ceil(len(lst) / n)\n",
    "  return list(\n",
    "    map(lambda x: lst[x * size:x * size + size],\n",
    "    list(range(n)))\n",
    "  )\n",
    "\n",
    "NUM_SHARDS = 10\n",
    "shards = list(chunk_into_n(teacher_files, NUM_SHARDS))\n",
    "train_filenames = itertools.chain.from_iterable(shards[:-1])\n",
    "test_filenames = itertools.chain.from_iterable(shards[-1:])\n",
    "\n",
    "TOKENIZER_DIR = pathlib.Path(\"../home_assistant_datasets/tokenizer\")\n",
    "TOKENIZER_CONFIG_JSON = \"tokenizer_config.json\"\n",
    "LLAMA3_TOKENIZER = TOKENIZER_DIR / \"llama3\" / TOKENIZER_CONFIG_JSON\n",
    "\n",
    "CONVERSATION = \"assist-llm-function-calling\"\n",
    "CONVERSATION_DIR = COLLECT_DIR / CONVERSATION\n",
    "MESSAGES = \"assist-llm-function-calling-messages\"\n",
    "MESSAGES_DIR = COLLECT_DIR / MESSAGES\n",
    "ASSIST_CHAT = \"assist-llm-function-calling-llama3-chat\"\n",
    "ASSIST_CHAT_DIR = COLLECT_DIR / ASSIST_CHAT\n",
    "\n",
    "shutil.rmtree(CONVERSATION_DIR, ignore_errors=True)\n",
    "CONVERSATION_DIR.mkdir(exist_ok=True)\n",
    "shutil.rmtree(MESSAGES_DIR, ignore_errors=True)\n",
    "MESSAGES_DIR.mkdir(exist_ok=True)\n",
    "shutil.rmtree(ASSIST_CHAT_DIR, ignore_errors=True)\n",
    "ASSIST_CHAT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "for split, filenames in [(\"train\", list(train_filenames)), (\"test\", list(test_filenames))]:\n",
    "    conversation_dir = CONVERSATION_DIR / split\n",
    "    conversation_dir.mkdir(exist_ok=True)\n",
    "    messages_dir = MESSAGES_DIR / split\n",
    "    messages_dir.mkdir(exist_ok=True)\n",
    "    assist_chat_dir = ASSIST_CHAT_DIR / split\n",
    "    assist_chat_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    conversation_file = conversation_dir / \"conversation.jsonl\"\n",
    "    messages_file = messages_dir / \"messages.jsonl\"\n",
    "    assist_chat_file = assist_chat_dir / \"chat.jsonl\"\n",
    "\n",
    "    with conversation_file.open(\"w\") as conversation_fd, messages_file.open(\"w\") as messages_fd, assist_chat_file.open(\"w\") as assist_chat_fd:\n",
    "        for filename in tqdm.tqdm(filenames, desc=split):\n",
    "            record = yaml.load(filename.read_text(), Loader=yaml.Loader)\n",
    "            conversation_record = create_conversation(record)\n",
    "\n",
    "            conversation_fd.write(conversation_record.to_json())\n",
    "            conversation_fd.write(\"\\n\")\n",
    "\n",
    "            messages_fd.write(conversation_record.to_messages_jsonl())\n",
    "            messages_fd.write(\"\\n\")\n",
    "\n",
    "            text = chat_template.build_prompt(\n",
    "                messages=conversation_record.to_messages(),\n",
    "                tools=conversation_record.tools,\n",
    "                add_generation_prompt=True,\n",
    "                tokenizer_config=LLAMA3_TOKENIZER,\n",
    "            )\n",
    "            json.dump({\"text\": text}, fp=assist_chat_fd)\n",
    "            assist_chat_fd.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load JSON from file '/workspaces/home-assistant-datasets/datasets/device-actions-v2-collect/assist-llm-function-calling/train/conversation.jsonl' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column(/tool_calls/[]/arguments/brightness) changed from string to number in row 5\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/packaged_modules/json/json.py:153\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    151\u001b[0m         file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mencoding_errors\n\u001b[1;32m    152\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 153\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/json/_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/json/_json.py:1025\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/json/_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/json/_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/json/_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1403\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m     )\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/builder.py:1997\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1996\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1997\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/packaged_modules/json/json.py:156\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    155\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load JSON from file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/packaged_modules/json/json.py:130\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpaj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReadOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pyarrow/_json.pyx:308\u001b[0m, in \u001b[0;36mpyarrow._json.read_json\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: JSON parse error: Column(/tool_calls/[]/arguments/brightness) changed from string to number in row 5",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m REPO_MAP \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massist-llm-function-calling\u001b[39m\u001b[38;5;124m\"\u001b[39m: CONVERSATION_DIR,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massist-llm-function-calling-messages\u001b[39m\u001b[38;5;124m\"\u001b[39m: MESSAGES_DIR,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massist-llm-function-calling-llama3-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m: ASSIST_CHAT_DIR,\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m repo_id, path \u001b[38;5;129;01min\u001b[39;00m REPO_MAP\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 11\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     ds\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallenporter/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, token\u001b[38;5;241m=\u001b[39msecrets\u001b[38;5;241m.\u001b[39mget_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_token\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/load.py:2616\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2616\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2625\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2626\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2627\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/builder.py:1029\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1028\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/builder.py:1124\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1124\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1131\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/builder.py:1884\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1882\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1883\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1884\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/builder.py:2040\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 2040\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from home_assistant_datasets import secrets\n",
    "\n",
    "REPO_MAP = {\n",
    "    \"assist-llm-function-calling\": CONVERSATION_DIR,\n",
    "    \"assist-llm-function-calling-messages\": MESSAGES_DIR,\n",
    "    \"assist-llm-function-calling-llama3-chat\": ASSIST_CHAT_DIR,\n",
    "}\n",
    "\n",
    "for repo_id, path in REPO_MAP.items():\n",
    "    ds = datasets.load_dataset(str(path))\n",
    "    ds.push_to_hub(f\"allenporter/{repo_id}\", token=secrets.get_secret(\"huggingface_token\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/allenporter/assist-llm-function-calling-llama3-chat/commit/f4b74970ec93cae58f70b3fb6c08ebaa83910f5d', commit_message='Upload README.md with huggingface_hub', commit_description='', oid='f4b74970ec93cae58f70b3fb6c08ebaa83910f5d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import DatasetCardData, DatasetCard\n",
    "\n",
    "readme = COLLECT_DIR / \"README.md\"\n",
    "\n",
    "for repo_id in REPO_MAP:\n",
    "    card = DatasetCard(content=readme.read_text())\n",
    "    card.push_to_hub(f\"allenporter/{repo_id}\", token=secrets.get_secret(\"huggingface_token\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"role\": \"system\", \"content\": \"You are a voice assistant for Home Assistant.\\nAnswer questions about the world truthfully.\\nAnswer in plain text. Keep it simple and to the point.\\nWhen controlling Home Assistant always call the intent tools. Use HassTurnOn to lock and HassTurnOff to unlock a lock. When controlling a device, prefer passing just name and domain. When controlling an area, prefer passing just area name and domain.\\nWhen a user asks to turn on all devices of a specific type, ask user to specify an area, unless there is only one device of that type.\\nThis device is not able to start timers.\\nAn overview of the areas and the devices in this smart home:\\n- names: Bathroom Exhaust Fan\\n  domain: fan\\n  state: 'off'\\n  areas: Bathroom\\n- names: Bedroom Light\\n  domain: light\\n  state: 'on'\\n  areas: Bedroom\\n  attributes:\\n    brightness: '100'\\n- names: Bathroom Light\\n  domain: light\\n  state: 'off'\\n  areas: Bathroom\\n- names: Living Room Light\\n  domain: light\\n  state: 'on'\\n  areas: Living Room\\n  attributes:\\n    brightness: '100'\\n- names: Kitchen Light\\n  domain: light\\n  state: 'on'\\n  areas: Kitchen\\n  attributes:\\n    brightness: '100'\\n- names: Balcony Light\\n  domain: light\\n  state: 'off'\\n  areas: Balcony\\n- names: Storage Room Light\\n  domain: light\\n  state: 'off'\\n  areas: Storage Room\\n- names: Smart Speaker\\n  domain: media_player\\n  state: playing\\n  areas: Living Room\\n  attributes:\\n    volume_level: 0.5\\n    device_class: speaker\\n- names: Bathroom Exhaust Fan Humidity\\n  domain: sensor\\n  state: '45'\\n  areas: Bathroom\\n  attributes:\\n    unit_of_measurement: '%'\\n    device_class: humidity\\n- names: Smart Plug Energy\\n  domain: sensor\\n  state: '1'\\n  areas: Kitchen\\n  attributes:\\n    unit_of_measurement: kWh\\n    device_class: energy\\n- names: Smart Plug\\n  domain: switch\\n  state: 'off'\\n  areas: Kitchen\\n  attributes:\\n    device_class: outlet\\n\", \"tool_calls\": []}, {\"role\": \"user\", \"content\": \"Please set the bedroom light to 50% brightness\", \"tool_calls\": []}, {\"role\": \"assistant\", \"content\": \"\", \"tool_calls\": [{\"name\": \"HassLightSet\", \"arguments\": {\"brightness\": 50.0, \"name\": \"Bedroom Light\"}}]}]\n",
      "{'role': 'system', 'content': \"You are a voice assistant for Home Assistant.\\nAnswer questions about the world truthfully.\\nAnswer in plain text. Keep it simple and to the point.\\nWhen controlling Home Assistant always call the intent tools. Use HassTurnOn to lock and HassTurnOff to unlock a lock. When controlling a device, prefer passing just name and domain. When controlling an area, prefer passing just area name and domain.\\nWhen a user asks to turn on all devices of a specific type, ask user to specify an area, unless there is only one device of that type.\\nThis device is not able to start timers.\\nAn overview of the areas and the devices in this smart home:\\n- names: Bathroom Exhaust Fan\\n  domain: fan\\n  state: 'off'\\n  areas: Bathroom\\n- names: Bedroom Light\\n  domain: light\\n  state: 'on'\\n  areas: Bedroom\\n  attributes:\\n    brightness: '100'\\n- names: Bathroom Light\\n  domain: light\\n  state: 'off'\\n  areas: Bathroom\\n- names: Living Room Light\\n  domain: light\\n  state: 'on'\\n  areas: Living Room\\n  attributes:\\n    brightness: '100'\\n- names: Kitchen Light\\n  domain: light\\n  state: 'on'\\n  areas: Kitchen\\n  attributes:\\n    brightness: '100'\\n- names: Balcony Light\\n  domain: light\\n  state: 'off'\\n  areas: Balcony\\n- names: Storage Room Light\\n  domain: light\\n  state: 'off'\\n  areas: Storage Room\\n- names: Smart Speaker\\n  domain: media_player\\n  state: playing\\n  areas: Living Room\\n  attributes:\\n    volume_level: 0.5\\n    device_class: speaker\\n- names: Bathroom Exhaust Fan Humidity\\n  domain: sensor\\n  state: '45'\\n  areas: Bathroom\\n  attributes:\\n    unit_of_measurement: '%'\\n    device_class: humidity\\n- names: Smart Plug Energy\\n  domain: sensor\\n  state: '1'\\n  areas: Kitchen\\n  attributes:\\n    unit_of_measurement: kWh\\n    device_class: energy\\n- names: Smart Plug\\n  domain: switch\\n  state: 'off'\\n  areas: Kitchen\\n  attributes:\\n    device_class: outlet\\n\", 'tool_calls': []}\n"
     ]
    }
   ],
   "source": [
    "f = \"../datasets/device-actions-v2-collect/assist-llm-function-calling/train/conversation.jsonl\"\n",
    "with open(f) as fd:\n",
    "    line = fd.readline()\n",
    "\n",
    "s = json.loads(line)\n",
    "print(s[\"messages\"])\n",
    "print(json.loads(s[\"messages\"])[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
